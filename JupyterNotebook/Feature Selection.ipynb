{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9cf023",
   "metadata": {},
   "source": [
    "## SU (Symmetrical Uncertainty)\n",
    "\n",
    "Symmetrical Uncertainty (SU) is a concept in information theory used to measure the relationship between two variables. The core idea of SU is based on information entropy and conditional entropy. Information entropy measures the uncertainty of a random variable, while conditional entropy represents the uncertainty of a random variable given another variable. SU assesses the relationship between two variables by comparing the ratio of these two information quantities.\n",
    "\n",
    "Specifically, for two random variables X and Y, with information entropies denoted as $H(X)$ and $H(Y)$ respectively, and conditional entropy denoted as $H(X|Y)$, the SU is calculated using the following formula:\n",
    "$$\n",
    "SU(X, Y) = \\frac{2 \\times (H(X) - H(X|Y))}{H(X) + H(Y)}\n",
    "$$\n",
    "Here, $H(X|Y)$ is the conditional entropy of X given Y. The SU values range from 0 to 1, where 0 indicates no relationship between the two variables, and 1 indicates a complete relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7086544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 18  3 14 29 47 11 48 32 36 20 51 37 19 17 49 35 50 26  4 22 12 46 24\n",
      " 21  8  5  7 39 15 13  6 34 23 44 38 40 31 42 16 25 10 33  2 43  1 41  0\n",
      " 52 27 30 28  9]\n"
     ]
    }
   ],
   "source": [
    "# calculation of SU\n",
    "import numpy as np\n",
    "from skfeature.utility.mutual_information import su_calculation\n",
    "\n",
    "# load data and normalize it\n",
    "data = np.load(\"../data/features_train.npy\")\n",
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "data = (data-mean)/std\n",
    "label = np.load(\"../data/simu_20000_0.1_90_140_train.npy\")[:,1004] #S\n",
    "\n",
    "# transform the data to fit the SU calculation algorithm\n",
    "data = data * 10000\n",
    "data = data.astype(int)\n",
    "\n",
    "su = []\n",
    "for i in range(data.shape[1]):\n",
    "    su = np.append(su, su_calculation(data[:,i], label))\n",
    "print(np.argsort(su)[::-1]) #descend 0 1 2 should be at the forefront"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f2c88",
   "metadata": {},
   "source": [
    "## FCBF (Fast Correlation-Based Filter)\n",
    "\n",
    "FCBF is a feature selection algorithm designed for efficiently selecting relevant features in high-dimensional datasets. The algorithm focuses on identifying features that exhibit high correlation with the target variable while minimizing redundancy among the selected features. Its advatanges are computaional efficiency and consideration of redundancy.\n",
    "\n",
    "### Steps:\n",
    "1.Symmetrical Uncertainty Calculation: Compute the Symmetrical Uncertainty (SU) for each feature with respect to the target variable. SU is an information-theoretic metric that quantifies the relationship between two variables while considering the entropy of both.\n",
    "\n",
    "2.Sort Features by SU: Rank features based on their SU values to determine their relevance to the target variable.\n",
    "\n",
    "3.Initialize Result Set: Create an initial set to store the ultimately selected features.\n",
    "\n",
    "4.Iterate Through Features: Select the feature with the highest SU from the sorted list and add it to the result set.\n",
    "\n",
    "5.Remove Feature Redundancy: For features already added to the result set, remove highly redundant features. Redundancy is measured by computing the SU between selected features.\n",
    "\n",
    "6.Repeat Selection and Removal: Iterate through the process of feature selection and redundancy removal until the desired number of features is selected or until no more features are available for selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd0727d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45]\n"
     ]
    }
   ],
   "source": [
    "# FCBF\n",
    "import numpy as np\n",
    "from skfeature.function.information_theoretical_based import FCBF\n",
    "\n",
    "# load data and normalize it\n",
    "data = np.load(\"../data/features_train.npy\")\n",
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "data = (data-mean)/std\n",
    "label = np.load(\"../data/simu_20000_0.1_90_140_train.npy\")[:,1004] #S\n",
    "\n",
    "# transform the data to fit the FCBF algorithm\n",
    "data = data * 10000\n",
    "data = data.astype(int)\n",
    "\n",
    "selected_features = FCBF.fcbf(data, label)[0]\n",
    "\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08369ad",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "\n",
    "Mutual information measures the dependency between the two variables, that is, the reduction in entropy after knowing the information of another variable. Compared with Pearson correlation & F-Score, it also captures non-linear relationships.\n",
    " \n",
    "  \n",
    "For discrete distributions (for both x and y):  \n",
    "$$\n",
    "I(x, y) = H(Y) - H(Y|X) = \\sum_{x\\in \\mathit{X}}  \\sum_{x\\in \\mathit{Y}} \\textit{p}_{(X,Y)}(x,y) \\textrm{log}(\\frac{\\textit{p}_{(X,Y)}(x,y)}{\\textit{p}_{X}(x)\\textit{p}_{Y}(y)})\n",
    "$$\n",
    "Where $\\textit{p}_{(X,Y)}(x,y)$ is the joint probability mass function (PMF) of x and y, $\\textit{p}_{X}(x)$ is the PMF of x.\n",
    "\n",
    "For continues distribution (for both x and y):  \n",
    "$$\n",
    "I(X, Y) = H(Y) - H(Y|X) = \\int_X \\int_Y  \\textit{p}_{(X,Y)}(x,y) \\textrm{log}(\\frac{\\textit{p}_{(X,Y)}(x,y)}{\\textit{p}_{X}(x)\\textit{p}_{Y}(y)}) \\, \\, dx dy \n",
    "$$\n",
    "Where $\\textit{p}_{(X,Y)}(x,y)$ is the joint probability density function (PDF) of x and y, $\\textit{p}_{X}(x)$ is the PDF of x. In the continues situation, we usually bin the continues data first then treat them as discrete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "389a1dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 15  1 41 13 48 34 18 29 32 31  3 25 45 17 50 49 47 36 16  7 28 19 44\n",
      " 42 26 22  6  8 39  4 14  2 46 33  5 40 12 35 23 11 51 37 21 24 52 27 43\n",
      " 38  9 20 30  0]\n"
     ]
    }
   ],
   "source": [
    "# calculation of MI\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import numpy as np\n",
    "\n",
    "# load data and normalize it\n",
    "data = np.load(\"../data/features_train.npy\")\n",
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "data = (data-mean)/std\n",
    "label = np.load(\"../data/simu_20000_0.1_90_140_train.npy\")[:,1004] #S\n",
    "\n",
    "# transform the data to fit the MI calculation algorithm\n",
    "data = data * 10000\n",
    "data = data.astype(int)\n",
    "\n",
    "mi = []\n",
    "for i in range(data.shape[1]):\n",
    "    mi = np.append(mi, mutual_info_regression(data[:,i].reshape(-1,1), label))\n",
    "print(np.argsort(mi)[::-1]) # 0 1 2 should at the forefront"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc404a",
   "metadata": {},
   "source": [
    "## MRMR (Max-Relevance Min-Redundancy)\n",
    "\n",
    "The mRMR method tries to find a subset of features that have a higher association (MI) with the target variable while at the same time have lower inter-association with all the features already in the subset. It is a step-wise method, at each step, the feature $X_i, (X_i \\notin  S)$ with the highest feature importance score $f^{mRMR}(X_i)$ will be added to the subset until reach desired number of features in the subset. \n",
    "\n",
    "Formula:  \n",
    "$$\n",
    "f^{mRMR}(X_i) = I(Y, X_i) - \\frac{1}{|S|}\\sum_{X_s \\in S} I(X_s, X_i)\n",
    "$$\n",
    "where $I(Y, X_i)$ is the MI between feature $X_i$ and target variable. $\\frac{1}{|S|}\\sum_{X_s \\in S} I(X_s, X_i)$ is the average MI between feature $X_i$ and all the features already in the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8c5c59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  9 28  2 20 27 43 48 52  1 47 30 41 37 33]\n"
     ]
    }
   ],
   "source": [
    "# mRMR\n",
    "import numpy as np\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "# load data and normalize it\n",
    "data = np.load(\"../data/features_train.npy\")\n",
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "data = (data-mean)/std\n",
    "label = np.load(\"../data/simu_20000_0.1_90_140_train.npy\")[:,1004] #S\n",
    "\n",
    "# transform the data to fit the FCBF algorithm\n",
    "data = data * 1000 #1000\n",
    "data = data.astype(int)\n",
    "\n",
    "selected_features = 15\n",
    "selected_features,_,_ = MRMR.mrmr(data, label, n_selected_features = selected_features)\n",
    "\n",
    "print(selected_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
